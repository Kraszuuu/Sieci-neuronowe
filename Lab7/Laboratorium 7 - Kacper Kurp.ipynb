{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Wczytanie i przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametry danych\n",
    "vocab_size = 10000  # Liczba najczęściej występujących słów\n",
    "max_len = 128       # Maksymalna długość sekwencji (przycinanie/padowanie)\n",
    "\n",
    "# Wczytanie danych IMDB\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Padowanie sekwencji\n",
    "x_train = pad_sequences(x_train, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "x_test = pad_sequences(x_test, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Konwersja danych na tensory PyTorch\n",
    "x_train, y_train = torch.tensor(x_train), torch.tensor(y_train)\n",
    "x_test, y_test = torch.tensor(x_test), torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 128])\n",
      "128\n",
      "tensor(8685, dtype=torch.int32)\n",
      "tensor([   1, 7619, 1160,    4,  801,  114,    7,    4,  402, 1355, 1179, 5351,\n",
      "         184,  185,  539,   11, 7623,   13,   28,    8,  202,   12,   56,   18,\n",
      "           4, 1723,   37,  343,    6, 4411,   18,   49,    7,    4,  402, 1320,\n",
      "        1189,  665,   25,  165,  104,   18,    6,  333,  225,  170,    8,   30,\n",
      "         489,   11,    4,  365,  149, 7619,   16,   40,  319,   35, 2374,  116,\n",
      "         707,  140,  143,   45, 2272,   56,   49,    7,    4,   91, 2094,  913,\n",
      "        3990, 5585,  139,   26,  256,   46,   19,    4, 8685,    7,    6,    2,\n",
      "           2,   13, 3550,  138,   36,  161,   43,  276,    4, 5435, 4320,   23,\n",
      "         370,   38,    4, 1507,  586,   28,    8, 1343, 6787,   68,    2,    4,\n",
      "         107,  293,  539,   71,  540, 2218,   18,   68,  830,    6, 2367,  247,\n",
      "          74,  676, 3779,    2,   17,    4,  293, 2284], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "number = randint(1, len(x_train) + 1)\n",
    "print(x_train.shape)\n",
    "print(len(x_train[number]))\n",
    "print(max(x_train[number]))\n",
    "print(x_train[number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Budowa modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, rnn_type):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)    # embedding slów\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)             # wartswa liniowa\n",
    "        self.activation = nn.Sigmoid()                          # funkcja aktywacji\n",
    "\n",
    "        if self.rnn_type == \"LSTM\":                             # warstwa rekurencyjna\n",
    "            self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'RNN'.\")\n",
    "\n",
    "    # RNN zwraca tylko dwa elementy: sekwencję wyjść i ostatni stan ukryty. W przypadku RNN nie występuje dodatkowy cel ukryty, jak w LSTM\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        if self.rnn_type == \"LSTM\": _, (hidden, _) = self.rnn(x)\n",
    "        elif self.rnn_type == \"RNN\": _, hidden = self.rnn(x)\n",
    "        else: raise ValueError(\"Unknown RNN type in forward method. Choose 'LSTM' or 'RNN'.\")\n",
    "        x = self.fc(hidden[-1])\n",
    "        return self.activation(x)  # Funkcja aktywacji (sigmoid)\n",
    "\n",
    "# Parametry modelu\n",
    "embed_dim = 64\n",
    "hidden_dim = 128\n",
    "output_dim = 1      # klasyfikacja binarna\n",
    "\n",
    "models = [SentimentRNN(vocab_size, embed_dim, hidden_dim, output_dim, rnn_type=\"RNN\"),\n",
    "          SentimentRNN(vocab_size, embed_dim, hidden_dim, output_dim, rnn_type=\"LSTM\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementacja funkcji treningu i ewaluacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Dataloader\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Funkcja treningu\n",
    "def train_model(model, train_loader, criterion = nn.BCELoss(), epochs = 2, learning_rate = 0.001):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x_batch).squeeze()\n",
    "            loss = criterion(predictions, y_batch.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        acc = evaluate_model(model, x_test, y_test)\n",
    "        print(f\"Model {model.rnn_type}, epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}, Acc: {acc * 100:.2f}%\")\n",
    "\n",
    "# Funkcja do ewaluacji\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(x_test).squeeze()\n",
    "        predictions = torch.round(predictions)\n",
    "        accuracy = (predictions == y_test).float().mean().item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Trening i ocena modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RNN, epoch 1, Loss: 0.6963, Acc: 51.96%\n",
      "Model RNN, epoch 2, Loss: 0.6908, Acc: 51.30%\n",
      "Model LSTM, epoch 1, Loss: 0.6860, Acc: 54.11%\n",
      "Model LSTM, epoch 2, Loss: 0.6810, Acc: 51.90%\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    train_model(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
