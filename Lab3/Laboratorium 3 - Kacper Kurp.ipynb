{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie danych\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Porzucenie linii z pustymi etykietami oraz odpowiadajacych im wartosci\n",
    "feature_matrix = heart_disease.data.features.dropna()\n",
    "labels = heart_disease.data.targets.loc[feature_matrix.index]\n",
    "\n",
    "# Przetworzenie zbioru wartości przewidywanych do wartości binarnych\n",
    "y_binary = labels.copy()\n",
    "y_binary['num'] = y_binary['num'].apply(lambda x: 1 if x != 0 else 0)                      \n",
    "\n",
    "# Utworznnie zbioru dummy etykiet\n",
    "x_dummy = pd.get_dummies(feature_matrix, columns=['cp', 'restecg', 'slope','ca','thal'])         \n",
    "\n",
    "# Podział danych na zbiór uczący i testowy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy, y_binary, test_size=0.2, random_state=268555)\n",
    "\n",
    "# Normalizacja cech trenignowych i testowych\n",
    "scaler = StandardScaler()\n",
    "x_train_normalised = scaler.fit_transform(x_train)\n",
    "x_test_normalised = scaler.transform(x_test)\n",
    "\n",
    "# Konwersja danych do wymaganego formatu\n",
    "x_train = np.array(x_train).astype(float)\n",
    "y_train = np.array(y_train).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje aktywacji i ich pochodne\n",
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja kosztu i jej pochodna\n",
    "def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-15) + (1 - y_true) * np.log(1 - y_pred + 1e-15))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa pojedynczej warstwy\n",
    "class SingleLayer:\n",
    "    def __init__(self,\n",
    "                 input_layer_size : int,\n",
    "                 output_layer_size : int,\n",
    "                 activation : Callable[[np.ndarray, np.ndarray], float],\n",
    "                 activation_derivative : Callable[[np.ndarray, np.ndarray], float],\n",
    "                 std_dev : float = 0.01\n",
    "                 )-> None:\n",
    "        self.weights : np.ndarray = np.random.randn(input_layer_size, output_layer_size) * std_dev\n",
    "        self.biases : np.ndarray = np.zeros((1, output_layer_size))\n",
    "        self.activation : Callable[[np.ndarray, np.ndarray], float] = activation\n",
    "        self.activation_derivative : Callable[[np.ndarray, np.ndarray], float] = activation_derivative\n",
    "    \n",
    "    # oblicza sume ważoną wejść,\n",
    "    # zapisuje feature_matrix do celow późniejszej propagacji wstecznej,\n",
    "    # zwraca wartości funkcji aktywacji\n",
    "    def forward(self, feature_matrix: np.ndarray) -> np.ndarray:\n",
    "        self.weighted_matrix = feature_matrix @ self.weights + self.biases\n",
    "        self.cache_x = feature_matrix\n",
    "        return self.activation(self.weighted_matrix)\n",
    "\n",
    "    # oblicza deltę, gradient wag i gradient biasów,\n",
    "    # zwraca iloczyn delta i transponowanych wag, który posłuży jako gradient dla poprzedniej warstwy\n",
    "    def backward(self, gradient: np.ndarray) -> np.ndarray:\n",
    "        delta = gradient * self.activation_derivative(self.weighted_matrix)\n",
    "        self.d_weights = self.cache_x.T @ delta\n",
    "        self.d_biases = np.sum(delta, axis=0, keepdims=True)\n",
    "        return delta @ self.weights.T\n",
    "\n",
    "    # akutalizacja wag\n",
    "    def update(self, learning_rate: float) -> None:\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa sieci neuronowej\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims: list[int], std_dev: float = 0.01) -> None:\n",
    "        self.layers: list[SingleLayer] = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            input_size: int = layer_dims[i]\n",
    "            output_size: int = layer_dims[i + 1]\n",
    "            if i < len(layer_dims) - 2:  # warstwy ukryte\n",
    "                self.layers.append(SingleLayer(input_size, output_size, relu, relu_derivative, std_dev))\n",
    "            else:  # ostatnia warstwa wyjściowa\n",
    "                self.layers.append(SingleLayer(input_size, output_size, sigmoid, sigmoid_derivative, std_dev))\n",
    "\n",
    "    # wykonanie sprzężenia w przód na wszystkich warstwach \n",
    "    def forward(self, feature_matrix: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            feature_matrix = layer.forward(feature_matrix)\n",
    "        return feature_matrix\n",
    "\n",
    "    # wykonanie propagacji wstecznej na wszystkich warstwach \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> None:\n",
    "        gradient: np.ndarray = binary_cross_entropy_derivative(y_true, y_pred)\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "    # wykonanie aktualizacji wag dla wszsytkich warstw\n",
    "    def update_weights(self, learning_rate: float) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    # przeszkolenie sieci przez określoną liczbę epok\n",
    "    def train(self, feature_matrix: np.ndarray, labels: np.ndarray, epochs = 1000, learning_rate_param = 0.01) -> None:\n",
    "        global learning_rate\n",
    "        learning_rate = learning_rate_param\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(feature_matrix)\n",
    "            cost = binary_cross_entropy(labels, y_pred)\n",
    "            self.backward(y_pred, labels)\n",
    "            self.update_weights(learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
    "\n",
    "    # predykcja wyniku\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.forward(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_set \u001b[38;5;129;01min\u001b[39;00m data_sets:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m std_dev \u001b[38;5;129;01min\u001b[39;00m std_devs:\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%s\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m%s\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m%s\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m%s\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m%s\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mstr\u001b[39m(single_learning_rate), \u001b[38;5;28mstr\u001b[39m(epoch), \u001b[38;5;28mstr\u001b[39m(data_set), \u001b[38;5;28mstr\u001b[39m(std_dev))\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# Tworzenie, trenowanie i testowanie modelu\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         model \u001b[38;5;241m=\u001b[39m NeuralNetwork(layer, std_dev)\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "layers = [[25, 1],\n",
    "          [25, 25, 1],\n",
    "          [25, 5, 1],\n",
    "          [25, 25, 5, 1],\n",
    "          [25, 12, 1],\n",
    "          [25, 8, 4, 1],\n",
    "          [25, 12, 6, 3, 1],\n",
    "          [25, 12, 8, 4, 1],\n",
    "          [25, 12, 8, 4, 2, 1]\n",
    "          ]\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "epochs_list = [1000, 5000, 10000]\n",
    "\n",
    "data_sets = [(x_train, x_test), (x_train_normalised, x_test_normalised)]\n",
    "\n",
    "std_devs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for layer in layers:\n",
    "        for single_learning_rate in learning_rates:\n",
    "            for epoch in epochs_list:\n",
    "                for data_set in data_sets:\n",
    "                    for std_dev in std_devs:\n",
    "                        print(f'%s\\t%s\\t%s\\t%s\\t%s\\t' % layer, single_learning_rate, epoch, data_set, std_dev)\n",
    "                        # Tworzenie, trenowanie i testowanie modelu\n",
    "                        model = NeuralNetwork(layer, std_dev)\n",
    "                        model.train(data_set[0], y_train)\n",
    "\n",
    "                        # Sprawdzenie predykcji\n",
    "                        predictions = model.predict(data_set[1])\n",
    "                        accuracy = np.mean(predictions == y_test)\n",
    "                        print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
