{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie danych\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Porzucenie linii z pustymi etykietami oraz odpowiadajacych im wartosci\n",
    "feature_matrix = heart_disease.data.features.dropna()\n",
    "labels = heart_disease.data.targets.loc[feature_matrix.index]\n",
    "\n",
    "# Przetworzenie zbioru wartości przewidywanych do wartości binarnych\n",
    "y_binary = labels.copy()\n",
    "y_binary['num'] = y_binary['num'].apply(lambda x: 1 if x != 0 else 0)                      \n",
    "\n",
    "# Utworznnie zbioru dummy etykiet\n",
    "x_dummy = pd.get_dummies(feature_matrix, columns=['cp', 'restecg', 'slope','ca','thal'])         \n",
    "\n",
    "# Podział danych na zbiór uczący i testowy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy, y_binary, test_size=0.2, random_state=268555)\n",
    "\n",
    "# Normalizacja cech trenignowych i testowych\n",
    "scaler = StandardScaler()\n",
    "x_train_normalised = scaler.fit_transform(x_train)\n",
    "x_test_normalised = scaler.transform(x_test)\n",
    "\n",
    "# Konwersja danych do wymaganego formatu\n",
    "x_train = np.array(x_train).astype(float)\n",
    "y_train = np.array(y_train).astype(float)\n",
    "\n",
    "x_test = np.array(x_test).astype(float)\n",
    "y_test = np.array(y_test).astype(float)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcje aktywacji i ich pochodne\n",
    "def relu(x: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja kosztu i jej pochodna\n",
    "def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa pojedynczej warstwy\n",
    "class SingleLayer:\n",
    "    def __init__(self,\n",
    "                 input_layer_size : int,\n",
    "                 output_layer_size : int,\n",
    "                 activation : Callable[[np.ndarray, np.ndarray], float],\n",
    "                 activation_derivative : Callable[[np.ndarray, np.ndarray], float],\n",
    "                 std_dev : float = 0.01\n",
    "                 )-> None:\n",
    "        self.weights : np.ndarray = np.random.randn(input_layer_size, output_layer_size) * std_dev\n",
    "        self.biases : np.ndarray = np.zeros((1, output_layer_size))\n",
    "        self.activation : Callable[[np.ndarray, np.ndarray], float] = activation\n",
    "        self.activation_derivative : Callable[[np.ndarray, np.ndarray], float] = activation_derivative\n",
    "    \n",
    "    # oblicza sume ważoną wejść,\n",
    "    # zapisuje feature_matrix do celow późniejszej propagacji wstecznej,\n",
    "    # zwraca wartości funkcji aktywacji\n",
    "    def forward(self, feature_matrix: np.ndarray) -> np.ndarray:\n",
    "        self.weighted_matrix = feature_matrix @ self.weights + self.biases\n",
    "        self.cache_x = feature_matrix\n",
    "        return self.activation(self.weighted_matrix)\n",
    "\n",
    "    # oblicza deltę, gradient wag i gradient biasów,\n",
    "    # zwraca iloczyn delta i transponowanych wag, który posłuży jako gradient dla poprzedniej warstwy\n",
    "    def backward(self, gradient: np.ndarray) -> np.ndarray:\n",
    "        delta = gradient * self.activation_derivative(self.weighted_matrix)\n",
    "        self.d_weights = self.cache_x.T @ delta\n",
    "        self.d_biases = np.sum(delta, axis=0, keepdims=True)\n",
    "        return delta @ self.weights.T\n",
    "\n",
    "    # akutalizacja wag\n",
    "    def update(self, learning_rate: float) -> None:\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klasa sieci neuronowej\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims: list[int], std_dev: float = 0.01) -> None:\n",
    "        self.layers: list[SingleLayer] = []\n",
    "        for i in range(len(layer_dims) - 1):\n",
    "            input_size: int = layer_dims[i]\n",
    "            output_size: int = layer_dims[i + 1]\n",
    "            if i < len(layer_dims) - 2:  # warstwy ukryte\n",
    "                self.layers.append(SingleLayer(input_size, output_size, relu, relu_derivative, std_dev))\n",
    "            else:  # ostatnia warstwa wyjściowa\n",
    "                self.layers.append(SingleLayer(input_size, output_size, sigmoid, sigmoid_derivative, std_dev))\n",
    "\n",
    "    # wykonanie sprzężenia w przód na wszystkich warstwach \n",
    "    def forward(self, feature_matrix: np.ndarray) -> np.ndarray:\n",
    "        for layer in self.layers:\n",
    "            feature_matrix = layer.forward(feature_matrix)\n",
    "        return feature_matrix\n",
    "\n",
    "    # wykonanie propagacji wstecznej na wszystkich warstwach \n",
    "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray) -> None:\n",
    "        gradient: np.ndarray = binary_cross_entropy_derivative(y_true, y_pred)\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer.backward(gradient)\n",
    "\n",
    "    # wykonanie aktualizacji wag dla wszsytkich warstw\n",
    "    def update_weights(self, learning_rate: float) -> None:\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    # przeszkolenie sieci przez określoną liczbę epok\n",
    "    def train(self, feature_matrix: np.ndarray, labels: np.ndarray, epochs = 1000, learning_rate_param = 0.01) -> None:\n",
    "        global learning_rate\n",
    "        learning_rate = learning_rate_param\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(feature_matrix)\n",
    "            cost = binary_cross_entropy(labels, y_pred)\n",
    "            self.backward(y_pred, labels)\n",
    "            self.update_weights(learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Cost: {cost}\")\n",
    "\n",
    "    # predykcja wyniku\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.forward(X) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6931681843515732\n",
      "Epoch 100, Cost: 0.6916420167138729\n",
      "Epoch 200, Cost: 0.691642016713873\n",
      "Epoch 300, Cost: 0.6916420167138729\n",
      "Epoch 400, Cost: 0.6916420167138729\n",
      "Epoch 500, Cost: 0.691642016713873\n",
      "Epoch 600, Cost: 0.6916420167138729\n",
      "Epoch 700, Cost: 0.6916420167138729\n",
      "Epoch 800, Cost: 0.691642016713873\n",
      "Epoch 900, Cost: 0.6916420167138729\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6931474019767426\n",
      "Epoch 100, Cost: 0.691642016713873\n",
      "Epoch 200, Cost: 0.6916420167138729\n",
      "Epoch 300, Cost: 0.6916420167138729\n",
      "Epoch 400, Cost: 0.691642016713873\n",
      "Epoch 500, Cost: 0.6916420167138729\n",
      "Epoch 600, Cost: 0.6916420167138729\n",
      "Epoch 700, Cost: 0.691642016713873\n",
      "Epoch 800, Cost: 0.6916420167138729\n",
      "Epoch 900, Cost: 0.6916420167138729\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6948479693476425\n",
      "Epoch 100, Cost: 0.6916420167138729\n",
      "Epoch 200, Cost: 0.691642016713873\n",
      "Epoch 300, Cost: 0.6916420167138729\n",
      "Epoch 400, Cost: 0.6916420167138729\n",
      "Epoch 500, Cost: 0.691642016713873\n",
      "Epoch 600, Cost: 0.6916420167138729\n",
      "Epoch 700, Cost: 0.6916420167138729\n",
      "Epoch 800, Cost: 0.691642016713873\n",
      "Epoch 900, Cost: 0.6916420167138729\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6930924023387264\n",
      "Epoch 100, Cost: 0.6916420167138729\n",
      "Epoch 200, Cost: 0.691642016713873\n",
      "Epoch 300, Cost: 0.691642016713873\n",
      "Epoch 400, Cost: 0.6916420167138729\n",
      "Epoch 500, Cost: 0.691642016713873\n",
      "Epoch 600, Cost: 0.691642016713873\n",
      "Epoch 700, Cost: 0.6916420167138729\n",
      "Epoch 800, Cost: 0.691642016713873\n",
      "Epoch 900, Cost: 0.691642016713873\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.693145159835868\n",
      "Epoch 100, Cost: 0.691642016713873\n",
      "Epoch 200, Cost: 0.6916420167138729\n",
      "Epoch 300, Cost: 0.691642016713873\n",
      "Epoch 400, Cost: 0.691642016713873\n",
      "Epoch 500, Cost: 0.6916420167138729\n",
      "Epoch 600, Cost: 0.691642016713873\n",
      "Epoch 700, Cost: 0.691642016713873\n",
      "Epoch 800, Cost: 0.6916420167138729\n",
      "Epoch 900, Cost: 0.691642016713873\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6931465432916217\n",
      "Epoch 100, Cost: 0.691642016713873\n",
      "Epoch 200, Cost: 0.6916420167138729\n",
      "Epoch 300, Cost: 0.6916420167138729\n",
      "Epoch 400, Cost: 0.691642016713873\n",
      "Epoch 500, Cost: 0.6916420167138729\n",
      "Epoch 600, Cost: 0.6916420167138729\n",
      "Epoch 700, Cost: 0.691642016713873\n",
      "Epoch 800, Cost: 0.6916420167138729\n",
      "Epoch 900, Cost: 0.6916420167138729\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6899004588920612\n",
      "Epoch 100, Cost: 8.705131823091415\n",
      "Epoch 200, Cost: 8.705131823091415\n",
      "Epoch 300, Cost: 8.705131823091415\n",
      "Epoch 400, Cost: 8.705131823091415\n",
      "Epoch 500, Cost: 8.705131823091415\n",
      "Epoch 600, Cost: 8.705131823091415\n",
      "Epoch 700, Cost: 8.705131823091415\n",
      "Epoch 800, Cost: 8.705131823091415\n",
      "Epoch 900, Cost: 8.705131823091415\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6931523467896775\n",
      "Epoch 100, Cost: 0.6916420167138729\n",
      "Epoch 200, Cost: 0.691642016713873\n",
      "Epoch 300, Cost: 0.691642016713873\n",
      "Epoch 400, Cost: 0.6916420167138729\n",
      "Epoch 500, Cost: 0.691642016713873\n",
      "Epoch 600, Cost: 0.691642016713873\n",
      "Epoch 700, Cost: 0.6916420167138729\n",
      "Epoch 800, Cost: 0.691642016713873\n",
      "Epoch 900, Cost: 0.691642016713873\n",
      "Accuracy: 0.5833333333333334\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Epoch 0, Cost: 0.6931468713371649\n",
      "Epoch 100, Cost: 0.17859829574041214\n",
      "Epoch 200, Cost: 0.016160398833952126\n",
      "Epoch 300, Cost: 0.004757990238989236\n",
      "Epoch 400, Cost: 0.0026487399970733573\n",
      "Epoch 500, Cost: 0.001320130673795872\n",
      "Epoch 600, Cost: 0.0010174773096171287\n",
      "Epoch 700, Cost: 0.0008459323677002332\n",
      "Epoch 800, Cost: 0.00072484076552931\n",
      "Epoch 900, Cost: 0.0006334538215675183\n",
      "Accuracy: 0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "layers = [[25, 1],\n",
    "          [25, 25, 1],\n",
    "          [25, 5, 1],\n",
    "          [25, 25, 5, 1],\n",
    "          [25, 12, 1],\n",
    "          [25, 8, 4, 1],\n",
    "          [25, 12, 6, 3, 1],\n",
    "          [25, 12, 8, 4, 1],\n",
    "          [25, 12, 8, 4, 2, 1]\n",
    "          ]\n",
    "\n",
    "learning_rates = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "epochs_list = [1000, 5000, 10000]\n",
    "\n",
    "data_sets = [(x_train, x_test), (x_train_normalised, x_test_normalised)]\n",
    "\n",
    "std_devs = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "\n",
    "# layers, std_dev, data_sets, learing_rate\n",
    "parameters = [\n",
    "              [[25, 8, 4, 1], 0.01, (x_train, x_test), 0.01],           # wartość bazowa\n",
    "\n",
    "              [[25, 12, 6, 2, 1], 0.01, (x_train, x_test), 0.01],       # inna ilosc warstw\n",
    "              [[25, 6, 1], 0.01, (x_train, x_test), 0.01],            \n",
    "\n",
    "              [[25, 12, 6, 1], 0.01, (x_train, x_test), 0.01],          # inna ilosc perceptonow w warstwach\n",
    "\n",
    "              [[25, 8, 4, 1], 0.01, (x_train, x_test), 0.05],           # inne wartosci wspolczynnika uczenia\n",
    "              [[25, 8, 4, 1], 0.01, (x_train, x_test), 0.005],\n",
    "\n",
    "              [[25, 8, 4, 1], 0.05, (x_train, x_test), 0.05],           # inne wartosci odchylenia\n",
    "              [[25, 8, 4, 1], 0.005, (x_train, x_test), 0.05],\n",
    "\n",
    "              [[25, 8, 4, 1], 0.01, (x_train_normalised, x_test_normalised), 0.01],           # dane znormalizowane\n",
    "\n",
    "             ]\n",
    "\n",
    "for param in parameters:\n",
    "    print(\"\\n=================================================================\\n\")\n",
    "    # Inicjalizacja sieci neuronowej\n",
    "    model = NeuralNetwork(param[0], param[1])\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    model.train(param[2][0], y_train)\n",
    "\n",
    "    # Testowanie modelu\n",
    "    predictions = model.predict(param[2][1])\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Najlepsze wyniki zostały otrzymane dzięki normalizacji danych"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
