{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie danych\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Porzucenie linii z pustymi etykietami oraz odpowiadajacych im wartosci\n",
    "feature_matrix = heart_disease.data.features.dropna()\n",
    "labels = heart_disease.data.targets.loc[feature_matrix.index]\n",
    "\n",
    "# Przetworzenie zbioru wartości przewidywanych do wartości binarnych\n",
    "y_binary = labels.copy()\n",
    "y_binary['num'] = y_binary['num'].apply(lambda x: 1 if x != 0 else 0)                      \n",
    "\n",
    "# Utworznnie zbioru dummy etykiet\n",
    "x_dummy = pd.get_dummies(feature_matrix, columns=['cp', 'restecg', 'slope','ca','thal'])   \n",
    "\n",
    "# upewnienie się co do typów wykorzystywanych danych\n",
    "x_dummy = x_dummy.fillna(0).astype(float)\n",
    "y_binary = y_binary.astype(float)\n",
    "\n",
    "# Podział danych na zbiór uczący i testowy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy, y_binary, test_size=0.2, random_state=268555)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Konwersja danych do tablic NumPy przed utworzeniem tensorów PyTorch\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)  # wektor kolumnowy\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)    # wektor kolumnowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stworzona klasa Dataset\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Instancja klasy Dataset\n",
    "train_dataset = HeartDiseaseDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = HeartDiseaseDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "# Stworzony model sieci neuronowej\n",
    "class HeartDiseaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, layer_one_dim = 0, layer_two_dim = 0, layer_three_dim = 0, layer_four_dim = 0):\n",
    "        super(HeartDiseaseModel, self).__init__()\n",
    "        self.layer_one_dim = layer_one_dim\n",
    "        self.layer_two_dim = layer_two_dim\n",
    "        self.layer_three_dim = layer_three_dim\n",
    "        self.layer_four_dim = layer_four_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        if (self.layer_one_dim == 0):    self.fc1 = nn.Linear(input_dim, 1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(input_dim, layer_one_dim)\n",
    "            if (self.layer_two_dim == 0):    self.fc2 = nn.Linear(layer_one_dim, 1)\n",
    "            else:\n",
    "                self.fc2 = nn.Linear(layer_one_dim, layer_two_dim)\n",
    "\n",
    "                if (self.layer_three_dim == 0):  self.fc3 = nn.Linear(layer_two_dim, 1)\n",
    "                else:\n",
    "                    self.fc3 = nn.Linear(layer_two_dim, layer_three_dim)\n",
    "\n",
    "                    if (self.layer_four_dim == 0):    self.fc4 = nn.Linear(layer_three_dim, 1)\n",
    "                    else:\n",
    "                        self.fc4 = nn.Linear(layer_three_dim, layer_four_dim)\n",
    "                        self.fc5 = nn.Linear(layer_four_dim, 1)\n",
    "                        \n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        if (self.layer_one_dim == 0):    x = self.sigmoid(x)\n",
    "        else:\n",
    "            x = self.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            if (self.layer_two_dim == 0):    x = self.sigmoid(x)\n",
    "            else:\n",
    "                x = self.relu(x)\n",
    "                x = self.fc3(x)\n",
    "                if (self.layer_three_dim == 0):  x = self.sigmoid(x)\n",
    "                else:\n",
    "                    x = self.relu(x)\n",
    "                    x = self.fc4(x)\n",
    "                    if (self.layer_four_dim != 0):\n",
    "                        x = self.relu(x)\n",
    "                        x = self.fc5(x)\n",
    "                    x = self.sigmoid(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Funkcja kosztu\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Trenowanie modelu\n",
    "def train_model(model, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_cost = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            labels = labels.squeeze(1)  # Usunięcie dodatkowego wymiaru\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_cost += loss.item()\n",
    "        \n",
    "        if(epoch % 10 == 9):\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Cost: {total_cost / len(train_loader):.4f}\")\n",
    "\n",
    "# Ewaluacja modelu\n",
    "def evaluate_model(model, loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > threshold).float()\n",
    "            all_preds.extend(preds.numpy().flatten())\n",
    "            all_labels.extend(labels.numpy().flatten())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=1)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "    return accuracy, f1, precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Ze względów praktycznych wartości parametrów różnią się względem poprzedniej listy, nie mniej jednak sprawdzono wpływ poszczególnych czynników na ogólne działanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, learning_rate, layer_one_dim, layer_two_dim, layer_three_dim, layer_four_dim\n",
    "parameters = [\n",
    "    # zmiany batch_size\n",
    "    [64, 0.001, 8, 4, 0, 0],\n",
    "    [32, 0.001, 8, 4, 0, 0],\n",
    "    [16, 0.001, 8, 4, 0, 0],\n",
    "\n",
    "    # zmiany learning_rate\n",
    "    [64, 0.01, 8, 4, 0, 0],\n",
    "    [64, 0.0001, 8, 4, 0, 0],\n",
    "\n",
    "    # zmiany w ilości perceptonów\n",
    "    [64, 0.001, 8, 4, 0, 0],\n",
    "    [64, 0.001, 12, 6, 2, 0],\n",
    "    [64, 0.001, 6, 0, 0, 0],\n",
    "    [64, 0.001, 12, 6, 0, 0],\n",
    "    [64, 0.001, 12, 8, 4, 2],\n",
    "    [64, 0.001, 4, 2, 0, 0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.7308\n",
      "Epoch [20/100], Cost: 0.7034\n",
      "Epoch [30/100], Cost: 0.6830\n",
      "Epoch [40/100], Cost: 0.6691\n",
      "Epoch [50/100], Cost: 0.6482\n",
      "Epoch [60/100], Cost: 0.6315\n",
      "Epoch [70/100], Cost: 0.6139\n",
      "Epoch [80/100], Cost: 0.5952\n",
      "Epoch [90/100], Cost: 0.5754\n",
      "Epoch [100/100], Cost: 0.5540\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.4899\n",
      "Epoch [20/100], Cost: 0.4289\n",
      "Epoch [30/100], Cost: 0.3824\n",
      "Epoch [40/100], Cost: 0.3564\n",
      "Epoch [50/100], Cost: 0.3242\n",
      "Epoch [60/100], Cost: 0.3185\n",
      "Epoch [70/100], Cost: 0.3029\n",
      "Epoch [80/100], Cost: 0.2929\n",
      "Epoch [90/100], Cost: 0.2851\n",
      "Epoch [100/100], Cost: 0.2686\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.1663\n",
      "Epoch [20/100], Cost: 0.1022\n",
      "Epoch [30/100], Cost: 0.0601\n",
      "Epoch [40/100], Cost: 0.0350\n",
      "Epoch [50/100], Cost: 0.0212\n",
      "Epoch [60/100], Cost: 0.0137\n",
      "Epoch [70/100], Cost: 0.0103\n",
      "Epoch [80/100], Cost: 0.0102\n",
      "Epoch [90/100], Cost: 0.0071\n",
      "Epoch [100/100], Cost: 0.0080\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 32\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6874\n",
      "Epoch [20/100], Cost: 0.6754\n",
      "Epoch [30/100], Cost: 0.6617\n",
      "Epoch [40/100], Cost: 0.6425\n",
      "Epoch [50/100], Cost: 0.6161\n",
      "Epoch [60/100], Cost: 0.5872\n",
      "Epoch [70/100], Cost: 0.5519\n",
      "Epoch [80/100], Cost: 0.5172\n",
      "Epoch [90/100], Cost: 0.4828\n",
      "Epoch [100/100], Cost: 0.4626\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 32\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.3917\n",
      "Epoch [20/100], Cost: 0.3432\n",
      "Epoch [30/100], Cost: 0.3023\n",
      "Epoch [40/100], Cost: 0.3132\n",
      "Epoch [50/100], Cost: 0.2903\n",
      "Epoch [60/100], Cost: 0.2557\n",
      "Epoch [70/100], Cost: 0.2537\n",
      "Epoch [80/100], Cost: 0.2229\n",
      "Epoch [90/100], Cost: 0.2407\n",
      "Epoch [100/100], Cost: 0.2223\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 32\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.1084\n",
      "Epoch [20/100], Cost: 0.0620\n",
      "Epoch [30/100], Cost: 0.0536\n",
      "Epoch [40/100], Cost: 0.0409\n",
      "Epoch [50/100], Cost: 0.0267\n",
      "Epoch [60/100], Cost: 0.0204\n",
      "Epoch [70/100], Cost: 0.0239\n",
      "Epoch [80/100], Cost: 0.0159\n",
      "Epoch [90/100], Cost: 0.0275\n",
      "Epoch [100/100], Cost: 0.0241\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 16\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.7017\n",
      "Epoch [20/100], Cost: 0.6807\n",
      "Epoch [30/100], Cost: 0.6553\n",
      "Epoch [40/100], Cost: 0.6133\n",
      "Epoch [50/100], Cost: 0.5542\n",
      "Epoch [60/100], Cost: 0.5001\n",
      "Epoch [70/100], Cost: 0.4569\n",
      "Epoch [80/100], Cost: 0.4195\n",
      "Epoch [90/100], Cost: 0.3874\n",
      "Epoch [100/100], Cost: 0.3597\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 16\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.3203\n",
      "Epoch [20/100], Cost: 0.2911\n",
      "Epoch [30/100], Cost: 0.2644\n",
      "Epoch [40/100], Cost: 0.2442\n",
      "Epoch [50/100], Cost: 0.2240\n",
      "Epoch [60/100], Cost: 0.2067\n",
      "Epoch [70/100], Cost: 0.1862\n",
      "Epoch [80/100], Cost: 0.1747\n",
      "Epoch [90/100], Cost: 0.1547\n",
      "Epoch [100/100], Cost: 0.1418\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 16\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.0939\n",
      "Epoch [20/100], Cost: 0.0348\n",
      "Epoch [30/100], Cost: 0.0195\n",
      "Epoch [40/100], Cost: 0.1450\n",
      "Epoch [50/100], Cost: 0.0145\n",
      "Epoch [60/100], Cost: 0.0125\n",
      "Epoch [70/100], Cost: 0.0105\n",
      "Epoch [80/100], Cost: 0.0078\n",
      "Epoch [90/100], Cost: 0.0573\n",
      "Epoch [100/100], Cost: 0.0071\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.01\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6839\n",
      "Epoch [20/100], Cost: 0.6285\n",
      "Epoch [30/100], Cost: 0.4499\n",
      "Epoch [40/100], Cost: 0.3471\n",
      "Epoch [50/100], Cost: 0.3139\n",
      "Epoch [60/100], Cost: 0.2902\n",
      "Epoch [70/100], Cost: 0.2730\n",
      "Epoch [80/100], Cost: 0.2569\n",
      "Epoch [90/100], Cost: 0.2462\n",
      "Epoch [100/100], Cost: 0.2250\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.01\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.1781\n",
      "Epoch [20/100], Cost: 0.1121\n",
      "Epoch [30/100], Cost: 0.0659\n",
      "Epoch [40/100], Cost: 0.0391\n",
      "Epoch [50/100], Cost: 0.0296\n",
      "Epoch [60/100], Cost: 0.0138\n",
      "Epoch [70/100], Cost: 0.0095\n",
      "Epoch [80/100], Cost: 0.0052\n",
      "Epoch [90/100], Cost: 0.0043\n",
      "Epoch [100/100], Cost: 0.0030\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.01\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.3790\n",
      "Epoch [20/100], Cost: 0.3055\n",
      "Epoch [30/100], Cost: 0.2667\n",
      "Epoch [40/100], Cost: 0.2507\n",
      "Epoch [50/100], Cost: 0.2448\n",
      "Epoch [60/100], Cost: 0.2764\n",
      "Epoch [70/100], Cost: 0.2015\n",
      "Epoch [80/100], Cost: 0.2049\n",
      "Epoch [90/100], Cost: 0.1975\n",
      "Epoch [100/100], Cost: 0.1928\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.0001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6904\n",
      "Epoch [20/100], Cost: 0.6913\n",
      "Epoch [30/100], Cost: 0.6892\n",
      "Epoch [40/100], Cost: 0.6866\n",
      "Epoch [50/100], Cost: 0.6868\n",
      "Epoch [60/100], Cost: 0.6880\n",
      "Epoch [70/100], Cost: 0.6862\n",
      "Epoch [80/100], Cost: 0.6868\n",
      "Epoch [90/100], Cost: 0.6858\n",
      "Epoch [100/100], Cost: 0.6844\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.0001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6850\n",
      "Epoch [20/100], Cost: 0.6819\n",
      "Epoch [30/100], Cost: 0.6760\n",
      "Epoch [40/100], Cost: 0.6728\n",
      "Epoch [50/100], Cost: 0.6671\n",
      "Epoch [60/100], Cost: 0.6627\n",
      "Epoch [70/100], Cost: 0.6563\n",
      "Epoch [80/100], Cost: 0.6516\n",
      "Epoch [90/100], Cost: 0.6435\n",
      "Epoch [100/100], Cost: 0.6367\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.0001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.4948\n",
      "Epoch [20/100], Cost: 0.4082\n",
      "Epoch [30/100], Cost: 0.3538\n",
      "Epoch [40/100], Cost: 0.3412\n",
      "Epoch [50/100], Cost: 0.3242\n",
      "Epoch [60/100], Cost: 0.3023\n",
      "Epoch [70/100], Cost: 0.2971\n",
      "Epoch [80/100], Cost: 0.2938\n",
      "Epoch [90/100], Cost: 0.2745\n",
      "Epoch [100/100], Cost: 0.2709\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6997\n",
      "Epoch [20/100], Cost: 0.6942\n",
      "Epoch [30/100], Cost: 0.6879\n",
      "Epoch [40/100], Cost: 0.6816\n",
      "Epoch [50/100], Cost: 0.6762\n",
      "Epoch [60/100], Cost: 0.6696\n",
      "Epoch [70/100], Cost: 0.6627\n",
      "Epoch [80/100], Cost: 0.6542\n",
      "Epoch [90/100], Cost: 0.6425\n",
      "Epoch [100/100], Cost: 0.6288\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.5533\n",
      "Epoch [20/100], Cost: 0.4637\n",
      "Epoch [30/100], Cost: 0.3921\n",
      "Epoch [40/100], Cost: 0.3438\n",
      "Epoch [50/100], Cost: 0.3245\n",
      "Epoch [60/100], Cost: 0.3057\n",
      "Epoch [70/100], Cost: 0.2939\n",
      "Epoch [80/100], Cost: 0.2895\n",
      "Epoch [90/100], Cost: 0.2791\n",
      "Epoch [100/100], Cost: 0.2779\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [8, 4, 0, 0]\n",
      "Epoch [10/100], Cost: 0.1398\n",
      "Epoch [20/100], Cost: 0.0804\n",
      "Epoch [30/100], Cost: 0.0515\n",
      "Epoch [40/100], Cost: 0.0388\n",
      "Epoch [50/100], Cost: 0.0253\n",
      "Epoch [60/100], Cost: 0.0163\n",
      "Epoch [70/100], Cost: 0.0082\n",
      "Epoch [80/100], Cost: 0.0069\n",
      "Epoch [90/100], Cost: 0.0074\n",
      "Epoch [100/100], Cost: 0.0063\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 2, 0]\n",
      "Epoch [10/100], Cost: 0.7222\n",
      "Epoch [20/100], Cost: 0.7137\n",
      "Epoch [30/100], Cost: 0.7126\n",
      "Epoch [40/100], Cost: 0.7066\n",
      "Epoch [50/100], Cost: 0.7049\n",
      "Epoch [60/100], Cost: 0.7016\n",
      "Epoch [70/100], Cost: 0.6996\n",
      "Epoch [80/100], Cost: 0.6978\n",
      "Epoch [90/100], Cost: 0.6957\n",
      "Epoch [100/100], Cost: 0.6947\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 2, 0]\n",
      "Epoch [10/100], Cost: 0.6637\n",
      "Epoch [20/100], Cost: 0.5861\n",
      "Epoch [30/100], Cost: 0.5243\n",
      "Epoch [40/100], Cost: 0.4988\n",
      "Epoch [50/100], Cost: 0.4739\n",
      "Epoch [60/100], Cost: 0.4606\n",
      "Epoch [70/100], Cost: 0.4476\n",
      "Epoch [80/100], Cost: 0.4265\n",
      "Epoch [90/100], Cost: 0.4113\n",
      "Epoch [100/100], Cost: 0.4075\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 2, 0]\n",
      "Epoch [10/100], Cost: 0.2347\n",
      "Epoch [20/100], Cost: 0.1705\n",
      "Epoch [30/100], Cost: 0.1635\n",
      "Epoch [40/100], Cost: 0.1591\n",
      "Epoch [50/100], Cost: 0.1461\n",
      "Epoch [60/100], Cost: 0.1446\n",
      "Epoch [70/100], Cost: 0.1555\n",
      "Epoch [80/100], Cost: 0.1544\n",
      "Epoch [90/100], Cost: 0.1424\n",
      "Epoch [100/100], Cost: 0.1463\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [6, 0, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6797\n",
      "Epoch [20/100], Cost: 0.6527\n",
      "Epoch [30/100], Cost: 0.6220\n",
      "Epoch [40/100], Cost: 0.5958\n",
      "Epoch [50/100], Cost: 0.5724\n",
      "Epoch [60/100], Cost: 0.5466\n",
      "Epoch [70/100], Cost: 0.5305\n",
      "Epoch [80/100], Cost: 0.5111\n",
      "Epoch [90/100], Cost: 0.4926\n",
      "Epoch [100/100], Cost: 0.4743\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [6, 0, 0, 0]\n",
      "Epoch [10/100], Cost: 0.4305\n",
      "Epoch [20/100], Cost: 0.3917\n",
      "Epoch [30/100], Cost: 0.3646\n",
      "Epoch [40/100], Cost: 0.3540\n",
      "Epoch [50/100], Cost: 0.3296\n",
      "Epoch [60/100], Cost: 0.3191\n",
      "Epoch [70/100], Cost: 0.3102\n",
      "Epoch [80/100], Cost: 0.3121\n",
      "Epoch [90/100], Cost: 0.2913\n",
      "Epoch [100/100], Cost: 0.2917\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [6, 0, 0, 0]\n",
      "Epoch [10/100], Cost: 0.2060\n",
      "Epoch [20/100], Cost: 0.1495\n",
      "Epoch [30/100], Cost: 0.1127\n",
      "Epoch [40/100], Cost: 0.0944\n",
      "Epoch [50/100], Cost: 0.0783\n",
      "Epoch [60/100], Cost: 0.0681\n",
      "Epoch [70/100], Cost: 0.0582\n",
      "Epoch [80/100], Cost: 0.0470\n",
      "Epoch [90/100], Cost: 0.0403\n",
      "Epoch [100/100], Cost: 0.0345\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6960\n",
      "Epoch [20/100], Cost: 0.6951\n",
      "Epoch [30/100], Cost: 0.6948\n",
      "Epoch [40/100], Cost: 0.6934\n",
      "Epoch [50/100], Cost: 0.6934\n",
      "Epoch [60/100], Cost: 0.6937\n",
      "Epoch [70/100], Cost: 0.6936\n",
      "Epoch [80/100], Cost: 0.6925\n",
      "Epoch [90/100], Cost: 0.6923\n",
      "Epoch [100/100], Cost: 0.6922\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6754\n",
      "Epoch [20/100], Cost: 0.6223\n",
      "Epoch [30/100], Cost: 0.5040\n",
      "Epoch [40/100], Cost: 0.3974\n",
      "Epoch [50/100], Cost: 0.3452\n",
      "Epoch [60/100], Cost: 0.3148\n",
      "Epoch [70/100], Cost: 0.2949\n",
      "Epoch [80/100], Cost: 0.2823\n",
      "Epoch [90/100], Cost: 0.2661\n",
      "Epoch [100/100], Cost: 0.2549\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 6, 0, 0]\n",
      "Epoch [10/100], Cost: 0.1295\n",
      "Epoch [20/100], Cost: 0.0571\n",
      "Epoch [30/100], Cost: 0.0374\n",
      "Epoch [40/100], Cost: 0.0290\n",
      "Epoch [50/100], Cost: 0.0307\n",
      "Epoch [60/100], Cost: 0.0238\n",
      "Epoch [70/100], Cost: 0.0241\n",
      "Epoch [80/100], Cost: 0.0201\n",
      "Epoch [90/100], Cost: 0.0210\n",
      "Epoch [100/100], Cost: 0.0206\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 8, 4, 2]\n",
      "Epoch [10/100], Cost: 0.7252\n",
      "Epoch [20/100], Cost: 0.7208\n",
      "Epoch [30/100], Cost: 0.7112\n",
      "Epoch [40/100], Cost: 0.7076\n",
      "Epoch [50/100], Cost: 0.7043\n",
      "Epoch [60/100], Cost: 0.7011\n",
      "Epoch [70/100], Cost: 0.6980\n",
      "Epoch [80/100], Cost: 0.6958\n",
      "Epoch [90/100], Cost: 0.6943\n",
      "Epoch [100/100], Cost: 0.6938\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 8, 4, 2]\n",
      "Epoch [10/100], Cost: 0.6864\n",
      "Epoch [20/100], Cost: 0.6686\n",
      "Epoch [30/100], Cost: 0.6231\n",
      "Epoch [40/100], Cost: 0.5139\n",
      "Epoch [50/100], Cost: 0.4523\n",
      "Epoch [60/100], Cost: 0.4073\n",
      "Epoch [70/100], Cost: 0.3554\n",
      "Epoch [80/100], Cost: 0.3089\n",
      "Epoch [90/100], Cost: 0.2769\n",
      "Epoch [100/100], Cost: 0.2537\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [12, 8, 4, 2]\n",
      "Epoch [10/100], Cost: 0.1479\n",
      "Epoch [20/100], Cost: 0.0811\n",
      "Epoch [30/100], Cost: 0.0533\n",
      "Epoch [40/100], Cost: 0.0271\n",
      "Epoch [50/100], Cost: 0.0159\n",
      "Epoch [60/100], Cost: 0.0100\n",
      "Epoch [70/100], Cost: 0.0096\n",
      "Epoch [80/100], Cost: 0.0064\n",
      "Epoch [90/100], Cost: 0.0054\n",
      "Epoch [100/100], Cost: 0.0052\n",
      "\n",
      "Training with SGD optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [4, 2, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6839\n",
      "Epoch [20/100], Cost: 0.6821\n",
      "Epoch [30/100], Cost: 0.6797\n",
      "Epoch [40/100], Cost: 0.6772\n",
      "Epoch [50/100], Cost: 0.6745\n",
      "Epoch [60/100], Cost: 0.6726\n",
      "Epoch [70/100], Cost: 0.6714\n",
      "Epoch [80/100], Cost: 0.6692\n",
      "Epoch [90/100], Cost: 0.6657\n",
      "Epoch [100/100], Cost: 0.6628\n",
      "\n",
      "Training with Adam optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [4, 2, 0, 0]\n",
      "Epoch [10/100], Cost: 0.6466\n",
      "Epoch [20/100], Cost: 0.6307\n",
      "Epoch [30/100], Cost: 0.6133\n",
      "Epoch [40/100], Cost: 0.5847\n",
      "Epoch [50/100], Cost: 0.5474\n",
      "Epoch [60/100], Cost: 0.5098\n",
      "Epoch [70/100], Cost: 0.4643\n",
      "Epoch [80/100], Cost: 0.4318\n",
      "Epoch [90/100], Cost: 0.3971\n",
      "Epoch [100/100], Cost: 0.3802\n",
      "\n",
      "Training with RMSprop optimizer.\n",
      "Batch: 64\n",
      "Learning: 0.001\n",
      "Network: [4, 2, 0, 0]\n",
      "Epoch [10/100], Cost: 0.2082\n",
      "Epoch [20/100], Cost: 0.1890\n",
      "Epoch [30/100], Cost: 0.1717\n",
      "Epoch [40/100], Cost: 0.1690\n",
      "Epoch [50/100], Cost: 0.1583\n",
      "Epoch [60/100], Cost: 0.1591\n",
      "Epoch [70/100], Cost: 0.1588\n",
      "Epoch [80/100], Cost: 0.1502\n",
      "Epoch [90/100], Cost: 0.1407\n",
      "Epoch [100/100], Cost: 0.1289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name:SGDBatch:64Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8166666666666667,\n",
       "  'f1_score': 0.7317073170731707,\n",
       "  'precision': 0.9375},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8833333333333333,\n",
       "  'f1_score': 0.8571428571428571,\n",
       "  'precision': 0.875},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.85,\n",
       "  'f1_score': 0.8301886792452831,\n",
       "  'precision': 0.7857142857142857},\n",
       " 'Name:SGDBatch:32Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.84,\n",
       "  'precision': 0.84},\n",
       " 'Name:AdamBatch:32Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.84,\n",
       "  'precision': 0.84},\n",
       " 'Name:RMSpropBatch:32Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.88,\n",
       "  'precision': 0.88},\n",
       " 'Name:SGDBatch:16Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.84,\n",
       "  'precision': 0.84},\n",
       " 'Name:AdamBatch:16Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.8833333333333333,\n",
       "  'f1_score': 0.8627450980392157,\n",
       "  'precision': 0.8461538461538461},\n",
       " 'Name:RMSpropBatch:16Learning:0.001Network:[8, 4, 0, 0]': {'accuracy': 0.85,\n",
       "  'f1_score': 0.8301886792452831,\n",
       "  'precision': 0.7857142857142857},\n",
       " 'Name:SGDBatch:64Learning:0.01Network:[8, 4, 0, 0]': {'accuracy': 0.8833333333333333,\n",
       "  'f1_score': 0.8571428571428571,\n",
       "  'precision': 0.875},\n",
       " 'Name:AdamBatch:64Learning:0.01Network:[8, 4, 0, 0]': {'accuracy': 0.8333333333333334,\n",
       "  'f1_score': 0.8076923076923077,\n",
       "  'precision': 0.7777777777777778},\n",
       " 'Name:RMSpropBatch:64Learning:0.01Network:[8, 4, 0, 0]': {'accuracy': 0.9166666666666666,\n",
       "  'f1_score': 0.8936170212765957,\n",
       "  'precision': 0.9545454545454546},\n",
       " 'Name:SGDBatch:64Learning:0.0001Network:[8, 4, 0, 0]': {'accuracy': 0.5833333333333334,\n",
       "  'f1_score': 0.0,\n",
       "  'precision': 1.0},\n",
       " 'Name:AdamBatch:64Learning:0.0001Network:[8, 4, 0, 0]': {'accuracy': 0.75,\n",
       "  'f1_score': 0.6153846153846154,\n",
       "  'precision': 0.8571428571428571},\n",
       " 'Name:RMSpropBatch:64Learning:0.0001Network:[8, 4, 0, 0]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.88,\n",
       "  'precision': 0.88},\n",
       " 'Name:SGDBatch:64Learning:0.001Network:[12, 6, 2, 0]': {'accuracy': 0.4166666666666667,\n",
       "  'f1_score': 0.5882352941176471,\n",
       "  'precision': 0.4166666666666667},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[12, 6, 2, 0]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.88,\n",
       "  'precision': 0.88},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[12, 6, 2, 0]': {'accuracy': 0.9333333333333333,\n",
       "  'f1_score': 0.9166666666666666,\n",
       "  'precision': 0.9565217391304348},\n",
       " 'Name:SGDBatch:64Learning:0.001Network:[6, 0, 0, 0]': {'accuracy': 0.8333333333333334,\n",
       "  'f1_score': 0.7916666666666666,\n",
       "  'precision': 0.8260869565217391},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[6, 0, 0, 0]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.88,\n",
       "  'precision': 0.88},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[6, 0, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.84,\n",
       "  'precision': 0.84},\n",
       " 'Name:SGDBatch:64Learning:0.001Network:[12, 6, 0, 0]': {'accuracy': 0.5833333333333334,\n",
       "  'f1_score': 0.0,\n",
       "  'precision': 1.0},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[12, 6, 0, 0]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.875,\n",
       "  'precision': 0.9130434782608695},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[12, 6, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.8461538461538461,\n",
       "  'precision': 0.8148148148148148},\n",
       " 'Name:SGDBatch:64Learning:0.001Network:[12, 8, 4, 2]': {'accuracy': 0.4166666666666667,\n",
       "  'f1_score': 0.5882352941176471,\n",
       "  'precision': 0.4166666666666667},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[12, 8, 4, 2]': {'accuracy': 0.9,\n",
       "  'f1_score': 0.8846153846153846,\n",
       "  'precision': 0.8518518518518519},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[12, 8, 4, 2]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.84,\n",
       "  'precision': 0.84},\n",
       " 'Name:SGDBatch:64Learning:0.001Network:[4, 2, 0, 0]': {'accuracy': 0.7,\n",
       "  'f1_score': 0.6666666666666666,\n",
       "  'precision': 0.6206896551724138},\n",
       " 'Name:AdamBatch:64Learning:0.001Network:[4, 2, 0, 0]': {'accuracy': 0.8666666666666667,\n",
       "  'f1_score': 0.8333333333333334,\n",
       "  'precision': 0.8695652173913043},\n",
       " 'Name:RMSpropBatch:64Learning:0.001Network:[4, 2, 0, 0]': {'accuracy': 0.9166666666666666,\n",
       "  'f1_score': 0.8936170212765957,\n",
       "  'precision': 0.9545454545454546}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trenowanie i ewaluacja z użyciem różnych optymalizatorów\n",
    "results = {}\n",
    "\n",
    "for parameter_set in  parameters:\n",
    "    # instancja DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=parameter_set[0], shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=parameter_set[0], shuffle=True)\n",
    "\n",
    "    # instancja modelu\n",
    "    model = HeartDiseaseModel(x_train_tensor.shape[1], parameter_set[2], parameter_set[3], parameter_set[4], parameter_set[5])\n",
    "\n",
    "    # optymalizatory\n",
    "    optimizers = {\n",
    "        \"SGD\": optim.SGD(model.parameters(), lr=parameter_set[1], momentum=0.9),\n",
    "        \"Adam\": optim.Adam(model.parameters(), lr=parameter_set[1]),\n",
    "        \"RMSprop\": optim.RMSprop(model.parameters(), lr=parameter_set[1], momentum=0.9)\n",
    "    }\n",
    "\n",
    "    for name, optimizer in optimizers.items():\n",
    "        print(f\"\\nTraining with {name} optimizer.\\nBatch: {parameter_set[0]}\\nLearning: {parameter_set[1]}\\nNetwork: [{parameter_set[2]}, {parameter_set[3]}, {parameter_set[4]}, {parameter_set[5]}]\")\n",
    "        train_model(model, optimizer)\n",
    "        accuracy, f1, precision = evaluate_model(model, test_loader)\n",
    "        results[\"Name:%sBatch:%sLearning:%sNetwork:%s\" % (name, parameter_set[0], parameter_set[1], [parameter_set[2], parameter_set[3], parameter_set[4], parameter_set[5]])] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1,\n",
    "            \"precision\": precision\n",
    "        }\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, filename='results.csv'):\n",
    "    headers = ['name', 'batch_size', 'learning_rate', 'neural_network', 'accuracy', 'f1_score', 'precision']\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        for key, metrics in results.items():\n",
    "            match = re.match(r\"Name:(.+)Batch:(\\d+)Learning:(\\d*\\.?\\d+)Network:\\[(.+)\\]\", key)\n",
    "            if match:\n",
    "                name, batch_size, learning_rate, neural_network = match.groups()\n",
    "                row = [\n",
    "                    name.strip(),\n",
    "                    int(batch_size),\n",
    "                    float(learning_rate),\n",
    "                    neural_network.strip(),\n",
    "                    metrics['accuracy'],\n",
    "                    metrics['f1_score'],\n",
    "                    metrics['precision']\n",
    "                ]\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Przykład użycia\n",
    "save_results_to_csv(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Wnioski: </h3>\n",
    "Zdecydowanie najlepszymi wynikami wyróżnia się optymalizator RMSprop, który potrafi osiągać wartości współczynników na poziomie 90%!. Nie zauważyłem znaczącej korelacji pomiędzy wartością parametrów uczenia i rozmiaru batcha a wynikami. Zarówno sieci z małą ilością warstw ukrytych i perceptonów, jak również ich odwrotności potrafiły poradzić sobie z zadaniem. Zaledwie 5 na 30 wyników osiągnęło średnią wartość parametrów na poziomie niższym niż 80% (wszystkie z nich korzystały z optymalizera SGD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
