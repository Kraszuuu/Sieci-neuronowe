{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Przygotowanie i podział danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importy\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobranie danych\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Porzucenie linii z pustymi etykietami oraz odpowiadajacych im wartosci\n",
    "feature_matrix = heart_disease.data.features.dropna()\n",
    "labels = heart_disease.data.targets.loc[feature_matrix.index]\n",
    "\n",
    "# Przetworzenie zbioru wartości przewidywanych do wartości binarnych\n",
    "y_binary = labels.copy()\n",
    "y_binary['num'] = y_binary['num'].apply(lambda x: 1 if x != 0 else 0)                      \n",
    "\n",
    "# Utworznnie zbioru dummy etykiet\n",
    "x_dummy = pd.get_dummies(feature_matrix, columns=['cp', 'restecg', 'slope','ca','thal'])         \n",
    "\n",
    "# Podział danych na zbiór uczący i testowy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy, y_binary, test_size=0.2, random_state=268555)\n",
    "\n",
    "# Normalizacja cech trenignowych i testowych\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Konwersja danych do wymaganego formatu\n",
    "x_train = np.array(x_train).astype(float)\n",
    "y_train = np.array(y_train).astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Definicja funkcji wykorzystywanych w zadaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Poniższa funkcja różni się od tej zaproponowanej w liście,\n",
    "    ze względu na błędy obliczeniowe przy ujemnych wartościach parametru z.\n",
    "    Funkcja where jest implementacją operatora ternarnego biblioteki numpy\n",
    "    \"\"\"\n",
    "    return np.where(z >= 0, \n",
    "                1 / (1 + np.exp(-z)), \n",
    "                np.exp(z) / (1 + np.exp(z)))\n",
    "\n",
    "def compute_gradient(feature_matrix, labels, weights):\n",
    "    \"\"\"\n",
    "    feature_matrix:  macierz wejściowa cech o wymiarze (ilość rekordów, ilość cech)\n",
    "    labels:         lista prawdziwych etykiet o długości równej ilości rekordów\n",
    "    weights:        lista wag poszczególnych cech. Długość jest równa ilości cech\n",
    "    \"\"\"\n",
    "    numeber_of_records = feature_matrix.shape[0]\n",
    "    gradients = np.zeros(weights.shape) # Inicjalizacja gradientów\n",
    "    \n",
    "    # Obliczanie prognoz modelu\n",
    "    z = np.matmul(feature_matrix, weights)\n",
    "    p = sigmoid(z)\n",
    "    \n",
    "    # Obliczanie gradientu dla każdej cechy\n",
    "    for i in range(len(weights)):\n",
    "        gradients[i] = -np.sum((labels - p) * feature_matrix[:, i]) / numeber_of_records\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# Funkcja gradientu spadku dla aktualizacji wag\n",
    "def gradient_descent(featureMatrix, labels, weights, learning_rate = 0.1, epochs = 1000, min_loss_change = 0.001):\n",
    "    \"\"\"\n",
    "    feature_matrix:     macierz wejściowa cech o wymiarze (ilość rekordów, ilość cech)\n",
    "    labels:             lista prawdziwych etykiet o długości równej ilości rekordów\n",
    "    weights:            lista wag poszczególnych cech. Długość jest równa ilości cech\n",
    "    learning_rate:      współczynnik uczenia\n",
    "    epochs:             liczba epok\n",
    "    min_loss_change:    minimalna różnica potrzebna do kontynuowania obliczeń\n",
    "    \"\"\"\n",
    "\n",
    "    prev_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        gradients = compute_gradient(featureMatrix, labels, weights)    # Obliczanie gradientów dla każdej epoki\n",
    "        weights -= learning_rate * gradients                            # Aktualizacja wag\n",
    "        \n",
    "        # Wyświetlanie informacji o postępie\n",
    "        if epoch % 100 == 0:\n",
    "            epsilon = 1e-5  # mała wartość dla uniknięcia log(0)\n",
    "            p = np.clip(sigmoid(featureMatrix @ weights), epsilon, 1 - epsilon) \n",
    "            \"\"\"\n",
    "            funkcja clip z zastosowaniem zmiennej epsilon zapobiega przekazaniu wartości 0 oraz 1,\n",
    "            do funkcji sigmoid, co zapobiega obliczaniu logarytmu z tych wartości.\n",
    "            \"\"\"\n",
    "            loss = np.mean(-labels * np.log(p) - (1 - labels) * np.log(1 - p))\n",
    "\n",
    "            print(f'Epoch {epoch}: Loss = {\"%.4f\" % loss}')\n",
    "\n",
    "            if (loss <= prev_loss + min_loss_change):\n",
    "                break\n",
    "\n",
    "            prev_loss = loss\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Trenowanie modelu oraz weryfikacja otrzymanych wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 4.2863\n",
      "Epoch 100: Loss = 4.9855\n",
      "Epoch 200: Loss = 5.0059\n",
      "Epoch 300: Loss = 5.0197\n",
      "Epoch 400: Loss = 5.0291\n",
      "Epoch 500: Loss = 5.0356\n",
      "Epoch 600: Loss = 5.0399\n",
      "Epoch 700: Loss = 5.0428\n",
      "Epoch 800: Loss = 5.0449\n",
      "Epoch 900: Loss = 5.0462\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.rand(x_train.shape[1]) # Inicjalizacja wag jako lista wartości z akresu 0-1\n",
    "\n",
    "# Trening modelu\n",
    "trained_w = gradient_descent(x_train, y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "F1 Score: 0.79\n",
      "Precision: 0.75\n",
      "Recall: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Prognozy prawdopodobieństwa\n",
    "y_pred = sigmoid(np.dot(x_test, trained_w))\n",
    "# Zamiana prawdopodobieństw na klasy (y < 0.5 => y = 0, y >= 0.5 => y = 1)\n",
    "y_pred = (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Wyliczenie metryk\n",
    "print(\"Accuracy:\", \"%.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", \"%.2f\" % f1_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
