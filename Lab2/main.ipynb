{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadaniem na zajęcia 2 jest implementacja działającego na zbiorze heart disease \n",
    "modelu klasyfikacji, przy czym: <br>\n",
    " \n",
    "• Zbieżność modelu można zdefiniować przez wystarczająco małą zmianę \n",
    "funkcji kosztu w danej iteracji i pewną maksymalną liczbę iteracji <br>\n",
    "• Model  może  uczyć  się  wyliczając  sumaryczną  pochodną  z  funkcji  kosztu  po \n",
    "całym zbiorze, po jednym przykładzie lub po paczce przykładów w iteracji. Dla \n",
    "sieci  neuronowej  w  następnym  zadaniu  będzie  już  wymagany  tryb \n",
    "paczkowania,  więc  warto  przećwiczyć  operacje  na  całych  macierzach  a  nie \n",
    "tylko pojedynczych wektorach danych <br>\n",
    "• Uczenie się modelu powinno być weryfikowalne metryką (np. accuracy, fscore, \n",
    "precision – można korzystać z bibliotek) <br>\n",
    "• Weryfikacja powinna uwzględnić podział na dane uczące i testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "\n",
    "# Porzucenie linii z pustymi etykietami oraz odpowiadajacych im wartosci\n",
    "x = heart_disease.data.features.dropna()\n",
    "y = heart_disease.data.targets.loc[x.index]\n",
    "\n",
    "# Utworzenie zbioru wartości binarnych i wartości w formie dummy\n",
    "y_binary = y.copy()\n",
    "y_binary['num'] = y_binary['num'].apply(lambda x: 1 if x != 0 else 0)\n",
    "y_dummy = pd.get_dummies(y, columns=['num'])\n",
    "\n",
    "# Utworznnie zbioru dummy etykiet\n",
    "x_dummy = pd.get_dummies(x, columns=['cp', 'restecg', 'slope','ca','thal'])\n",
    "x_dummy_binary = pd.get_dummies(x_dummy, columns=['sex', 'fbs', 'exang'])\n",
    "\n",
    "# Implementacja standardowej funkcji logistycznej jako funckji aktywacji\n",
    "def activation_function(n):\n",
    "    return 1 / (1 + np.exp(-n))\n",
    "\n",
    "def function_p(x, W=None, b=None):\n",
    "    return activation_function(W*x + b)\n",
    "\n",
    "# Implementacja funkcji kosztu na podstawie entropii krzyzowej\n",
    "def cost_function(x,y):\n",
    "    -y*np.log(function_p(x)) - (1 - y)*np.log(1 - function_p(x))\n",
    "\n",
    "# Backup function\n",
    "def compute_cost(y, y_hat):\n",
    "    m = len(y)\n",
    "    cost = (-1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9000\n",
      "Precision: 0.9005\n",
      "F1 Score: 0.8992\n"
     ]
    }
   ],
   "source": [
    "# Podział danych na zbiór uczący i testowy\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_dummy_binary, y_binary, test_size=0.3, random_state=268555)\n",
    "\n",
    "# Normalizacja cech\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Model regresji logistycznej\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Trenowanie modelu na danych uczących\n",
    "model.fit(x_train, np.ravel(y_train))   # funkcja ravel zmiania macierz o wymiarach nx1 na macierz o wymiarach 1xn\n",
    "\n",
    "# Predykcja na zbiorze testowym\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Weryfikacja za pomocą metryk\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 0.6931\n",
      "Iteration 100: Cost 0.4403\n",
      "Iteration 200: Cost 0.3895\n",
      "Iteration 300: Cost 0.3696\n",
      "Iteration 400: Cost 0.3589\n",
      "Iteration 500: Cost 0.3521\n",
      "Iteration 600: Cost 0.3473\n",
      "Iteration 700: Cost 0.3437\n",
      "Iteration 800: Cost 0.3409\n",
      "Iteration 900: Cost 0.3386\n",
      "Accuracy: 0.9000\n",
      "Precision: 0.9091\n",
      "F1 Score: 0.8696\n"
     ]
    }
   ],
   "source": [
    "# Funkcja sigmoidalna\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Funkcja kosztu (entropia krzyżowa)\n",
    "def compute_cost(y, y_hat):\n",
    "    m = len(y)\n",
    "    cost = (-1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost\n",
    "\n",
    "# Pochodna funkcji kosztu (gradient dla wag i biasu)\n",
    "def gradient_descent(x, y, learning_rate, iterations):\n",
    "    m, n = x.shape\n",
    "    W = np.zeros(n)  # Wagi\n",
    "    b = 0  # Bias\n",
    "    costs = []  # Do zapisywania wartości funkcji kosztu\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Predykcje modelu\n",
    "        z = np.dot(x, W) + b\n",
    "        y_hat = sigmoid(z)\n",
    "        \n",
    "        # Obliczanie gradientów\n",
    "        dW = (1 / m) * np.dot(x.T, (y_hat - y))\n",
    "        db = (1 / m) * np.sum(y_hat - y)\n",
    "        \n",
    "        # Aktualizacja wag\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Obliczanie i zapis kosztu co 100 iteracji\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(y, y_hat)\n",
    "            costs.append(cost)\n",
    "            print(f\"Iteration {i}: Cost {cost:.4f}\")\n",
    "    \n",
    "    return W, b, costs\n",
    "\n",
    "# Funkcja predykcji\n",
    "def predict(x, W, b):\n",
    "    z = np.dot(x, W) + b\n",
    "    y_hat = sigmoid(z)\n",
    "    return np.where(y_hat >= 0.5, 1, 0)\n",
    "\n",
    "# Trening modelu\n",
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "W, b, costs = gradient_descent(x_train, np.ravel(y_train), learning_rate, iterations)\n",
    "\n",
    "# Predykcja na zbiorze testowym\n",
    "y_pred = predict(x_test, W, b)\n",
    "\n",
    "# Ewaluacja modelu\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
